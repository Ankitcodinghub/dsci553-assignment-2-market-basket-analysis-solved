# dsci553-assignment-2-market-basket-analysis-solved
**TO GET THIS SOLUTION VISIT:** [DSCI553 Assignment 2-Market Basket Analysis Solved](https://www.ankitcodinghub.com/product/dsci553-assignment-2-market-basket-analysis-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;97058&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;1&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (1 vote)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;DSCI553 Assignment 2-Market Basket Analysis Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (1 vote)    </div>
    </div>
<div class="page" title="Page 1">
<div class="section">
<div class="layoutArea">
<div class="column">
1. Overview of the Assignment

In this assignment, you will implement the ​SON​ ​Algorithm​ ​using the Spark Framework. You will develop a program to find frequent itemsets in two datasets, one simulated dataset and one real-world generated dataset. The goal of this assignment is to apply the algorithms you have learned in class on large datasets more efficiently in a distributed environment.

2. Requirements

2.1 Programming Requirements

a. You must use ​Python​ to implement all tasks. You can only use standard python libraries (i.e., external libraries like numpy or pandas are not allowed). There will be a 10% bonus for each task if you also submit a Scala implementation and both your Python and Scala implementations are correct.

b. You are required to only use Spark RDD in order to understand Spark operations. You will not get any point if you use Spark DataFrame or DataSet.

2.2 Programming Environment

Python 3.6, JDK 1.8, Scala 2.11, and Spark 2.4.4

We will use these library versions to compile and test your code. There will be no point if we cannot run your code on Vocareum.

On Vocareum, you can call `spark-submit` located at `/home/local/spark/latest/bin/spark-submit`.

(*Do not use the one at /usr/local/bin/spark-submit (2.3.0))

2.3 Write your own code

Do not share code with other students!!

For this assignment to be an effective learning experience, you must write your own code! We emphasize this point because you will be able to find Python implementations of some of the required functions on the web. Please do not look for or at any such code!

TAs will combine all the code we can find from the web (e.g., Github) as well as other students’ code from this and other (previous) sections for plagiarism detection. We will report all detected plagiarism.

2.4 What you need to turn in

We will grade all submissions on Vocareum and the submissions on the blackboard will be ignored. Vocareum produces a submission report after you click the “Submit” button (​It takes a while since Vocareum needs to run your code in order to generate the report​). Vocareum will only grade Python scripts during the submission phase and it will grade both Python and Scala during the grading phase.

</div>
</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="section">
<div class="layoutArea">
<div class="column">
a. Two Python scripts, named: (all lowercase)

task1.py, task2.py

b. [OPTIONAL] hw2.jar and two Scala scripts, named: (all lowercase)

hw2.jar, task1.scala, task2.scala

c. You don’t need to include your results or the datasets. We will grade on your code with our testing data (data will be in the same format).

3. Datasets

In this assignment, you will use one simulated dataset and one real-world. In task 1, you will build and test your program with a small simulated CSV file that has been provided to you.

Then in task2 you need to generate a subset using the Ta Feng dataset (​https://bit.ly/2miWqFS​) with a structure similar to the simulated data.

Figure 1 shows the file structure of task1 simulated csv, the first column is user_id and the second column is business_id.

Figure 1: Input Data Format

4. Tasks

In this assignment, you will implement ​SON Algorithm ​to solve all tasks (Task 1 and 2) on top of Spark Framework. You need to find ​all the possible combinations of the frequent itemsets ​in any given input file within the required time. You can refer to the Chapter 6 from the Mining of Massive Datasets book and concentrate on section 6.4 – Limited-Pass Algorithms. (Hint: you can choose either A-Priori, MultiHash, or PCY algorithm to process each chunk of the data)

4.1 Task 1: Simulated data (3 pts)

There are two CSV files (small1.csv and small2.csv) on the Blackboard. The small1.csv is just a test file that you can use to debug your code. For task1, ​we will only test your code on small2.csv​.

In this task, you need to build two kinds of market-basket models.

Case 1 (1.5 pts)​:

You will calculate the combinations of frequent businesses (as singletons, pairs, triples, etc.) that are qualified as frequent given a support threshold. You need to create a basket for each user containing the business ids reviewed by this user. If a business was reviewed more than once by a reviewer, we consider this product was rated only once. More specifically, the business ids within each basket are unique. The generated baskets are similar to:

user1: [business11, business12, business13, …] user2: [business21, business22, business23, …] user3: [business31, business32, business33, …]

</div>
</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
Case 2 (1.5 pts)​:

You will calculate the combinations of frequent users (as singletons, pairs, triples, etc.) that are qualified as frequent given a support threshold. You need to create a basket for each business containing the user ids that commented on this business. Similar to case 1, the user ids within each basket are unique. The generated baskets are similar to:

business1: [user11, user12, user13, …] business2: [user21, user22, user23, …] business3: [user31, user32, user33, …]

Input format:

1. Case number: ​Integer ​that specifies the case. ​1 for Case 1 and 2 for Case 2​.

2. Support: ​Integer ​that defines the minimum count to qualify as a frequent itemset.

3. Input file path: This is the path to the input file including path, file name and extension.

4. Output file path: This is the path to the output file including path, file name and extension.

Output format:

1. Runtime: ​the total execution time from loading the file till finishing writing the output file ​You need to ​print the runtime in the console ​with the “Duration” tag, e.g., “Duration: 100”.

2. Output file:

(1) Intermediate result

You should use “Candidates:”as the tag. For each line you should output the candidates of frequent itemsets you found after the first pass of ​SON Algorithm ​followed by an empty line after each combination. The printed itemsets must be sorted in ​lexicographical ​order (Both user_id and business_id are type of string).

(2) Final result

You should use “Frequent Itemsets:”as the tag. For each line you should output the final frequent itemsets you found after finishing the ​SON Algorithm​. The format is the same with the intermediate results. The printed itemsets must be sorted in ​lexicographical ​order.

Here is an example of the output file:

</div>
</div>
<div class="layoutArea">
<div class="column">
Both the intermediate results and final results should be saved in ONE output result file.

Execution example:

Python: spark-submit task1.py &lt;case number&gt; &lt;support&gt; &lt;input_file_path&gt; &lt;output_file_path&gt; Scala: ​spark-submit –class task2 hw2.jar ​&lt;case number&gt; &lt;support&gt; &lt;input_file_path&gt; &lt;output_file_path&gt;

​4.2 Task 2: Ta Feng data (4 pts)

In task 2, you will explore the Ta Feng dataset to find the frequent itemsets (​only case 1​). You will use data found here from Kaggle (​https://bit.ly/2miWqFS​) to find product IDs associated with a given

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
customer ID each day. Aggregate all purchases a customer makes within a day into one basket. In other words, assume a customer purchases at once all items purchased within a day.

N.B.: Be careful when reading the csv file, as spark can read the product id numbers with leading zeros. You can manually format Column F (PRODUCT_ID) to numbers (with zero decimal places) in the csv file before reading it using spark.

SON Algorithm on Ta Feng data:

You will create a data pipeline where the ​input is the raw Ta Feng data​, and the ​output is the file described under “output file”​. You will pre-process the data, and then from this pre-processed data, you will create the final output. Your code is allowed to output this pre-processed data during execution, but you should ​NOT ​submit homework that includes this pre-processed data.

(1) Data preprocessing

You need to generate a dataset from the Ta Feng dataset with following steps:

1. Find the date of the purchase (column TRANSACTION_DT), such as December 1, 2000 (12/1/00)

2. At each date, select “CUSTOMER_ID” and “PRODUCT_ID”.

3. We want to consider all items bought by a consumer each day as a separate transaction (i.e., “baskets”). For example, if consumer 1, 2, and 3 each bought oranges December 2, 2000, and consumer 2 also bought celery on December 3, 2000, we would consider that to be 4 separate transactions. An easy way to do this is to rename each CUSTOMER_ID as “DATE-CUSTOMER_ID”. For example, if COSTOMER_ID is 12321, and this customer bought apples November 14, 2000, then their new ID is “11/14/00-12321”

4. Make sure each line in the CSV file is “DATE-CUSTOMER_ID1, PRODUCT_ID1”.

5. The header of CSV file should be “DATE-CUSTOMER_ID, PRODUCT_ID”

You need to save the dataset in CSV format. Figure below shows an example of the output file ​(please note DATE-CUSTOMER_ID and PRODUCT_ID are strings and integers, respectively)

Figure: customer_product file

Do ​NOT ​submit the output file of this data preprocessing step, but your code is allowed to create this file.

(2) Apply SON Algorithm

The requirements for task 2 are similar to task 1. However, you will test your implementation with the large dataset you just generated. For this purpose, you need to report the total execution time. For this execution time, we take into account also the time from reading the file till writing the results to the output file. You are asked to find the candidate and frequent itemsets (​similar to the previous task​) using the file you just generated. The following are the steps you need to do:

1. Reading the customer_product CSV file in to RDD and then build the case 1 market-basket model; 2. Find out qualified customers-date who purchased more than ​k i​ tems. (​k ​is the filter threshold);

3. Apply the ​SON Algorithm ​code to the filtered market-basket model;

Input format:

1. Filter threshold: ​Integer that is ​used to filter out qualified users

2. Support: ​Integer ​that defines the minimum count to qualify as a frequent itemset.

3. Input file path: This is the path to the input file including path, file name and extension.

4. Output file path: This is the path to the output file including path, file name and extension.

Output format:

1. Runtime: ​the total execution time from loading the file till finishing writing the output file ​You need to ​print the runtime in the console ​with the “Duration” tag, e.g., “Duration: 100”.

</div>
</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<div class="layoutArea">
<div class="column">
2. Output file

The output file format is the same with task 1. Both the intermediate results and final results should be saved in ONE output result file.

Execution example:

Python: spark-submit task2.py &lt;filter threshold&gt; &lt;support&gt; &lt;input_file_path&gt; &lt;output_file_path&gt; Scala:​ ​spark-submit –class task2 hw2.jar​ ​&lt;filter threshold&gt; &lt;support&gt; &lt;input_file_path&gt; &lt;output_file_path&gt;

</div>
</div>
<div class="layoutArea">
<div class="column">
6. Evaluation Metric Task 1:

Input File small2.csv small2.csv

Task 2:

Input File Customer_product.csv

</div>
</div>
</div>
</div>
